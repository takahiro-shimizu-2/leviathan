LangGraph + CrewAI + LangChainãŒYAMLé§†å‹•ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«æœ€é©ãªç†ç”±ã‚’ã€æŠ€è¡“çš„ãªæ·±æ˜ã‚Šã§è§£èª¬ã—ã¾ã™ã€‚
ğŸ¯ 1. LangGraph: çŠ¶æ…‹ç®¡ç†ã¨ãƒ•ãƒ­ãƒ¼åˆ¶å¾¡ã®å„ªä½æ€§
ãªãœYAMLçµ±åˆã«å„ªã‚Œã‚‹ã®ã‹
LangGraphã¯æœ‰å‘ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€ã“ã‚ŒãŒYAMLã®éšå±¤æ§‹é€ ã¨å®Œç’§ã«å¯¾å¿œã—ã¾ã™ã€‚
YAMLã®æ§‹é€ ã¨LangGraphã®å¯¾å¿œé–¢ä¿‚
yaml# manifest.yaml
workflow:
  nodes:
    - id: supreme_orchestrator
      type: supervisor
      next: [requirement_analysis, tech_selection]
    
    - id: requirement_analysis
      type: agent
      conditional_edges:
        - condition: needs_clarification
          target: user_input
        - condition: complete
          target: execution_orchestrator
    
    - id: execution_orchestrator
      type: parallel_crew
      agents: [architect, coder, tester]
LangGraphã¸ã®ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°
pythonfrom langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import Literal, TypedDict, Annotated
import operator

class SystemState(TypedDict):
    user_input: str
    requirements: Annotated[list, operator.add]  # è¿½è¨˜å‹ãƒªã‚¹ãƒˆ
    tech_stack: dict
    code_artifacts: dict
    current_phase: str
    errors: Annotated[list, operator.add]

def yaml_to_langgraph(manifest: dict) -> StateGraph:
    """YAMLã‹ã‚‰LangGraphã‚’å‹•çš„ç”Ÿæˆ"""
    workflow = StateGraph(SystemState)
    
    # YAMLã®ãƒãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾LangGraphãƒãƒ¼ãƒ‰ã«å¤‰æ›
    for node_config in manifest['workflow']['nodes']:
        node_id = node_config['id']
        node_type = node_config['type']
        
        # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé–¢æ•°ç”Ÿæˆ
        if node_type == 'supervisor':
            func = create_supervisor_node(node_config)
        elif node_type == 'agent':
            func = create_agent_node(node_config)
        elif node_type == 'parallel_crew':
            func = create_crew_node(node_config)
        
        workflow.add_node(node_id, func)
    
    # YAMLã®ã‚¨ãƒƒã‚¸ã‚’LangGraphã‚¨ãƒƒã‚¸ã«å¤‰æ›
    for node_config in manifest['workflow']['nodes']:
        if 'next' in node_config:
            for next_node in node_config['next']:
                workflow.add_edge(node_config['id'], next_node)
        
        # æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸ï¼ˆã“ã‚ŒãŒå¼·åŠ›ï¼‰
        if 'conditional_edges' in node_config:
            workflow.add_conditional_edges(
                node_config['id'],
                lambda state: evaluate_condition(state, node_config),
                {edge['condition']: edge['target'] 
                 for edge in node_config['conditional_edges']}
            )
    
    return workflow.compile(checkpointer=MemorySaver())
LangGraphã®æ±ºå®šçš„å„ªä½æ€§
â‘  çŠ¶æ…‹ã®æ™‚é–“è»¸ç®¡ç†ï¼ˆCheckpointingï¼‰
python# å®Ÿè¡Œé€”ä¸­ã®çŠ¶æ…‹ã‚’ä¿å­˜ãƒ»å¾©å…ƒå¯èƒ½
checkpointer = MemorySaver()
app = workflow.compile(checkpointer=checkpointer)

# å®Ÿè¡Œ
config = {"configurable": {"thread_id": "mvp-session-001"}}
result = app.invoke(initial_state, config)

# é€”ä¸­ã§ä¸­æ–­ã—ã¦ã‚‚ã€åŒã˜thread_idã§å†é–‹å¯èƒ½
# ã“ã‚Œã«ã‚ˆã‚ŠYAMLå®šç¾©ã®ã€Œé•·æ™‚é–“å®Ÿè¡Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€ã‚’å®Ÿç¾
continued = app.invoke(None, config)  # å‰å›ã®ç¶šãã‹ã‚‰
â‘¡ æ¡ä»¶åˆ†å²ã®æŸ”è»Ÿæ€§
pythondef route_based_on_requirements(state: SystemState) -> Literal["simple_mvp", "complex_mvp", "clarify"]:
    """Supreme Orchestratorã®åˆ¤æ–­ã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    if not state['requirements']:
        return "clarify"
    elif state['requirements']['complexity'] < 5:
        return "simple_mvp"
    else:
        return "complex_mvp"

workflow.add_conditional_edges(
    "supreme_orchestrator",
    route_based_on_requirements,
    {
        "simple_mvp": "quick_builder_agent",
        "complex_mvp": "execution_orchestrator",
        "clarify": "user_input_agent"
    }
)
ã“ã‚Œã«ã‚ˆã‚Šã€YAMLã§å®šç¾©ã—ãŸåˆ¤æ–­ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‹•çš„ã«å®Ÿè¡Œã§ãã¾ã™ã€‚
â‘¢ ä¸¦åˆ—å®Ÿè¡Œã¨ãƒãƒ¼ã‚¸
yaml# YAMLå®šç¾©
execution_orchestrator:
  parallel_agents:
    - frontend_dev
    - backend_dev
    - database_designer
  merge_strategy: wait_all
  merge_node: integration_agent
pythonfrom langgraph.graph import START

# è‡ªå‹•çš„ã«ä¸¦åˆ—å®Ÿè¡Œã•ã‚Œã‚‹
workflow.add_node("frontend_dev", frontend_agent)
workflow.add_node("backend_dev", backend_agent)
workflow.add_node("database_designer", db_agent)

# è¤‡æ•°ãƒãƒ¼ãƒ‰ã‹ã‚‰ä¸€ã¤ã®ãƒãƒ¼ãƒ‰ã¸ï¼ˆè‡ªå‹•ãƒãƒ¼ã‚¸ï¼‰
workflow.add_edge("frontend_dev", "integration_agent")
workflow.add_edge("backend_dev", "integration_agent")
workflow.add_edge("database_designer", "integration_agent")

ğŸ¤– 2. CrewAI: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿ã®æœ€é©åŒ–
ãªãœYAMLå®šç¾©ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æœ€é©ã‹
CrewAIã¯å½¹å‰²ãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­è¨ˆã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€YAMLå®šç¾©ã¨æ¦‚å¿µçš„ã«ä¸€è‡´ã—ã¾ã™ã€‚
YAMLå®šç¾©ã‹ã‚‰Crewã¸ã®å¤‰æ›
yaml# agents.yaml
agents:
  - name: architect
    role: "Solution Architect"
    goal: "ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¨­è¨ˆã‚’è¡Œã†"
    backstory: |
      15å¹´ã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºé–‹ç™ºçµŒé¨“ã‚’æŒã¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã€‚
      ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ã‚’é‡è¦–ã€‚
    tools:
      - tech_stack_analyzer
      - diagram_generator
    llm_config:
      model: "claude-sonnet-4.5"
      temperature: 0.3
  
  - name: coder
    role: "Full Stack Developer"
    goal: "æ©Ÿèƒ½è¦ä»¶ã‚’å®Ÿè£…ã™ã‚‹"
    backstory: "TypeScriptã€Pythonã€Rustã«ç²¾é€šã—ãŸé–‹ç™ºè€…"
    tools:
      - code_generator
      - git_manager
    llm_config:
      model: "claude-sonnet-4.5"
      temperature: 0.7

tasks:
  - id: design_phase
    description: "ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆ"
    assigned_to: architect
    expected_output: "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã¨APIä»•æ§˜"
  
  - id: implementation
    description: "è¨­è¨ˆã«åŸºã¥ã„ã¦å®Ÿè£…"
    assigned_to: coder
    depends_on: [design_phase]
    expected_output: "å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹"
CrewAIã¸ã®å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°
pythonfrom crewai import Agent, Task, Crew, Process
from langchain_anthropic import ChatAnthropic
import yaml

def create_crew_from_yaml(yaml_path: str) -> Crew:
    """YAMLã‹ã‚‰å®Œå…¨ãªCrewã‚’å‹•çš„ç”Ÿæˆ"""
    with open(yaml_path) as f:
        config = yaml.safe_load(f)
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”Ÿæˆ
    agents = []
    agent_map = {}
    
    for agent_config in config['agents']:
        llm = ChatAnthropic(
            model=agent_config['llm_config']['model'],
            temperature=agent_config['llm_config']['temperature']
        )
        
        agent = Agent(
            role=agent_config['role'],
            goal=agent_config['goal'],
            backstory=agent_config['backstory'],
            tools=[tool_registry[t] for t in agent_config['tools']],
            llm=llm,
            verbose=True,
            allow_delegation=True  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å§”è­²ã‚’è¨±å¯
        )
        
        agents.append(agent)
        agent_map[agent_config['name']] = agent
    
    # ã‚¿ã‚¹ã‚¯ç”Ÿæˆï¼ˆä¾å­˜é–¢ä¿‚ã‚‚è‡ªå‹•è§£æ±ºï¼‰
    tasks = []
    for task_config in config['tasks']:
        task = Task(
            description=task_config['description'],
            agent=agent_map[task_config['assigned_to']],
            expected_output=task_config['expected_output'],
            context=resolve_dependencies(task_config, tasks)  # ä¾å­˜ã‚¿ã‚¹ã‚¯æ³¨å…¥
        )
        tasks.append(task)
    
    # Crewçµ„ã¿ç«‹ã¦
    return Crew(
        agents=agents,
        tasks=tasks,
        process=Process.sequential,  # ã¾ãŸã¯ hierarchical
        verbose=True,
        memory=True,  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§è¨˜æ†¶å…±æœ‰
        embedder={
            "provider": "openai",
            "config": {"model": "text-embedding-3-small"}
        }
    )

# å®Ÿè¡Œ
crew = create_crew_from_yaml("agents.yaml")
result = crew.kickoff()
CrewAIã®æ±ºå®šçš„å„ªä½æ€§
â‘  éšå±¤çš„ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆHierarchical Processï¼‰
ã“ã‚ŒãŒSupreme Orchestrator â†’ Execution Orchestratorã®å®Ÿè£…ã«æœ€é©ï¼š
python# Supreme Orchestrator = Manager Agent
supreme_crew = Crew(
    agents=[architect, coder, tester],
    tasks=tasks,
    process=Process.hierarchical,  # ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒè‡ªå‹•çš„ã«ã‚¿ã‚¹ã‚¯å§”è­²
    manager_llm=ChatAnthropic(model="claude-sonnet-4.5"),
    verbose=True
)
ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆSupreme Orchestratorï¼‰ãŒè‡ªå‹•çš„ã«ï¼š

ã‚¿ã‚¹ã‚¯ã®å„ªå…ˆé †ä½ä»˜ã‘
æœ€é©ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®å‰²ã‚Šå½“ã¦
é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
ã‚¨ãƒ©ãƒ¼æ™‚ã®å†å‰²ã‚Šå½“ã¦

â‘¡ ãƒ¡ãƒ¢ãƒªã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå…±æœ‰
pythoncrew = Crew(
    agents=agents,
    tasks=tasks,
    memory=True,  # çŸ­æœŸãƒ»é•·æœŸãƒ¡ãƒ¢ãƒªæœ‰åŠ¹åŒ–
    verbose=True
)

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆAã®æ±ºå®šã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆBãŒå‚ç…§å¯èƒ½
# "architectãŒé¸ã‚“ã æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã«åŸºã¥ã„ã¦ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"ãŒè‡ªå‹•å®Ÿç¾
â‘¢ ãƒ„ãƒ¼ãƒ«ã®è‡ªå‹•å§”è­²
python@tool
def complex_calculation(data: dict) -> float:
    """è¤‡é›‘ãªè¨ˆç®—ã‚’å®Ÿè¡Œ"""
    return sum(data.values()) * 1.5

architect_agent = Agent(
    role="Architect",
    tools=[complex_calculation],
    allow_delegation=True
)

coder_agent = Agent(
    role="Coder",
    tools=[],  # ãƒ„ãƒ¼ãƒ«ã‚’æŒãŸãªã„
    allow_delegation=True
)

# coderãŒè¨ˆç®—ãŒå¿…è¦ã«ãªã£ãŸã‚‰ã€è‡ªå‹•çš„ã«architectã«å§”è­²

ğŸ”— 3. LangChain: ãƒ„ãƒ¼ãƒ«çµ±åˆã¨ãƒ¡ãƒ¢ãƒªã®è¦
ãªãœã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¸å¯æ¬ ã‹
LangChainã¯ãƒ„ãƒ¼ãƒ«ã¨ãƒ¡ãƒ¢ãƒªã®æ¨™æº–åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚
YAMLå®šç¾©ãƒ„ãƒ¼ãƒ«ã®å‹•çš„ãƒ­ãƒ¼ãƒ‰
yaml# tools.yaml
tools:
  - name: code_generator
    type: langchain_tool
    implementation: custom_tools.CodeGeneratorTool
    config:
      output_dir: "./generated"
      languages: [python, typescript, rust]
  
  - name: web_search
    type: langchain_community
    provider: tavily
    api_key_env: TAVILY_API_KEY
  
  - name: github_manager
    type: langchain_community
    provider: github
    operations: [create_repo, push_code, create_pr]
å‹•çš„ãƒ„ãƒ¼ãƒ«ãƒ­ãƒ¼ãƒ‰å®Ÿè£…
pythonfrom langchain.tools import Tool, StructuredTool
from langchain_community.tools import TavilySearchResults
from langchain_community.tools.github import GitHubAction
import importlib

def load_tools_from_yaml(yaml_path: str) -> list:
    """YAMLã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã‚’å‹•çš„ãƒ­ãƒ¼ãƒ‰"""
    with open(yaml_path) as f:
        config = yaml.safe_load(f)
    
    tools = []
    for tool_config in config['tools']:
        if tool_config['type'] == 'langchain_tool':
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã‚’å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            module_path, class_name = tool_config['implementation'].rsplit('.', 1)
            module = importlib.import_module(module_path)
            ToolClass = getattr(module, class_name)
            tools.append(ToolClass(**tool_config['config']))
        
        elif tool_config['type'] == 'langchain_community':
            if tool_config['provider'] == 'tavily':
                tools.append(TavilySearchResults())
            elif tool_config['provider'] == 'github':
                tools.append(GitHubAction())
    
    return tools

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ³¨å…¥
tools = load_tools_from_yaml("tools.yaml")
agent = Agent(role="Developer", tools=tools)
LangChainã®æ±ºå®šçš„å„ªä½æ€§
â‘  ãƒ¡ãƒ¢ãƒªã®æ°¸ç¶šåŒ–ã¨æ¤œç´¢
pythonfrom langchain.memory import ConversationBufferMemory, VectorStoreRetrieverMemory
from langchain_community.vectorstores import Chroma
from langchain_anthropic import AnthropicEmbeddings

# Supreme Orchestratorã®æ±ºå®šå±¥æ­´ã‚’ä¿å­˜
memory = VectorStoreRetrieverMemory(
    retriever=Chroma(
        embedding_function=AnthropicEmbeddings()
    ).as_retriever(search_kwargs=dict(k=5))
)

# å¾Œç¶šã®Execution OrchestratorãŒå‚ç…§å¯èƒ½
memory.save_context(
    {"input": "ECã‚µã‚¤ãƒˆã®MVPè¦æ±‚"},
    {"output": "Next.js + FastAPI + PostgreSQLæ§‹æˆã‚’æ¡ç”¨"}
)

# é¡ä¼¼ã®æ±ºå®šã‚’æ¤œç´¢
relevant_decisions = memory.load_memory_variables(
    {"input": "Eã‚³ãƒãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ§‹ç¯‰"}
)
â‘¡ ãƒ„ãƒ¼ãƒ«ã®ãƒã‚§ãƒ¼ãƒ³åŒ–
pythonfrom langchain.chains import SequentialChain, TransformChain

# YAMLã§å®šç¾©ã—ãŸãƒ„ãƒ¼ãƒ«ã‚’é€£é–å®Ÿè¡Œ
yaml_definition = """
tool_chain:
  - analyze_requirements  # è¦ä»¶åˆ†æ
  - select_tech_stack     # æŠ€è¡“é¸å®š
  - generate_scaffold     # ã‚¹ã‚­ãƒ£ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ç”Ÿæˆ
  - write_tests           # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
  - deploy_demo           # ãƒ‡ãƒ¢å±•é–‹
"""

# è‡ªå‹•çš„ã«ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
chain = create_chain_from_yaml(yaml_definition, tools)
result = chain.run(user_input="åœ¨åº«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦")
â‘¢ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥
pythonfrom langchain.agents import AgentType, initialize_agent

# YAMLå®šç¾©
"""
agent_strategy:
  type: STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION
  max_iterations: 15
  early_stopping_method: generate
"""

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    max_iterations=15,
    early_stopping_method="generate",
    handle_parsing_errors=True  # YAMLã§å®šç¾©ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
)

ğŸ”„ 4. ä¸‰è€…ã®çµ±åˆ: æœ€å¼·ã®ç†ç”±
ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å…¨ä½“åƒ
python# å®Œå…¨çµ±åˆä¾‹
from langgraph.graph import StateGraph
from crewai import Crew, Agent, Task
from langchain.tools import Tool
from langchain.memory import ConversationBufferMemory

class UnifiedOrchestrator:
    def __init__(self, manifest_path: str):
        self.manifest = self.load_yaml(manifest_path)
        
        # LangChain: ãƒ„ãƒ¼ãƒ«ã¨ãƒ¡ãƒ¢ãƒªå±¤
        self.tools = load_tools_from_yaml(self.manifest['tools'])
        self.memory = self.setup_memory()
        
        # CrewAI: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿å±¤
        self.crews = self.build_crews()
        
        # LangGraph: ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¤
        self.workflow = self.build_workflow()
    
    def build_crews(self) -> dict:
        """éšå±¤çš„Crewæ§‹ç¯‰"""
        crews = {}
        
        # Supreme Orchestrator Crew
        supreme_agent = Agent(
            role="Supreme Orchestrator",
            goal="ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ„æ€æ±ºå®š",
            tools=self.tools,
            memory=self.memory,
            llm=ChatAnthropic(model="claude-sonnet-4.5")
        )
        
        # Execution Orchestrator Crewï¼ˆéšå±¤ä¸‹ä½ï¼‰
        execution_agents = [
            Agent(role=cfg['role'], goal=cfg['goal'], tools=self.tools)
            for cfg in self.manifest['execution_agents']
        ]
        
        crews['supreme'] = Crew(
            agents=[supreme_agent],
            process=Process.hierarchical
        )
        
        crews['execution'] = Crew(
            agents=execution_agents,
            process=Process.hierarchical,
            manager_agent=supreme_agent  # Supreme ãŒç®¡ç†
        )
        
        return crews
    
    def build_workflow(self) -> StateGraph:
        """LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰"""
        workflow = StateGraph(SystemState)
        
        # Supreme Orchestrator ãƒãƒ¼ãƒ‰
        workflow.add_node(
            "supreme",
            lambda state: self.crews['supreme'].kickoff(inputs=state)
        )
        
        # Execution Orchestrator ãƒãƒ¼ãƒ‰
        workflow.add_node(
            "execute",
            lambda state: self.crews['execution'].kickoff(inputs=state)
        )
        
        # æ¡ä»¶ä»˜ããƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆLangGraphã®å¼·ã¿ï¼‰
        workflow.add_conditional_edges(
            "supreme",
            self.route_decision,
            {
                "proceed": "execute",
                "clarify": "user_input",
                "abort": END
            }
        )
        
        return workflow.compile()
    
    def route_decision(self, state: SystemState) -> str:
        """Supreme Orchestrator ã®åˆ¤æ–­ã‚’è§£é‡ˆ"""
        decision = state.get('supreme_decision', {})
        
        if decision.get('confidence', 0) > 0.8:
            return "proceed"
        elif decision.get('needs_clarification'):
            return "clarify"
        else:
            return "abort"